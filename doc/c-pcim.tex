\documentclass[12pt]{article}

\usepackage[top=1.00in, bottom=1.25in, left=0.75in, right=0.75in]{geometry}

\usepackage{amsmath, amssymb, amsthm}

\usepackage[round]{natbib}

\usepackage{url}
\usepackage{breakurl}
\usepackage[breaklinks]{hyperref}
\def\UrlBreaks{\do\/\do-}

\usepackage[ruled,lined]{algorithm2e}

\newcommand{\argmin}[1]{\underset{#1}{\operatorname{argmin}}\ }
\newcommand{\argmax}[1]{\underset{#1}{\operatorname{argmax}}\ }

\theoremstyle{plain}
\newtheorem*{theorem}{Theorem}
\newtheorem*{lemma}{Lemma}
\newtheorem*{corollary}{Corollary}

\theoremstyle{definition}
\newtheorem{definition}{Definition}
\newtheorem*{convention}{Convention}

\theoremstyle{remark}
\newtheorem*{notation}{Notation}
\newtheorem*{note}{Note}
\newtheorem*{claim}{Claim}

\newcommand{\suchthat}{\mid}
\newcommand{\given}{\mid}


\begin{document}

\begin{definition}
  Data that is counted is \textbf{discrete} while data that is measured is
  \textbf{continuous} \citep{matisfundis18}.
\end{definition}

\begin{definition}
  A \textbf{combination} is an arrangement of items without regard to the order.
  A \textbf{permutation} is an arrangement of items in a specific order.
  \citep{matisfuncom18}.
\end{definition}

\begin{definition}
  An \textbf{experiment} is a procedure that yields one of a given set of
  possible outcomes \citep{ros02}.
\end{definition}

\begin{definition}
  The \textbf{sample space} of the experiment is the set of possible outcomes
  \citep{ros02}.
\end{definition}

\begin{definition}
  An \textbf{event} is a subset of the sample space \citep{ros02}.
\end{definition}

\begin{definition}
  \textbf{Probability} is the measure of the likelihood that an event will
  occur.
  Probability is quantified as a number between 0 and 1, where 0 indicates
  impossibility and 1 certainty.
\end{definition}

\begin{definition}
  \textbf{Finite probability}. If $S$ is a finite nonempty sample space of
  equally likely outcomes, and $E$ is an event, that is, a subset of $S$,
  then the probability of $E$ is $p(E)=\frac{|E|}{|S|}$ \citep{ros02}.
\end{definition}

\begin{definition}
  A \textbf{random variable}\footnotemark\ is a function from the sample space
  of an experiment to the set of real numbers.
  \begin{equation*}
    X:S \to \mathbb{R}
  \end{equation*}
  A random variable $X(t)$ maps each outcome $t$ to a real number \citep{ros02}.
  \footnotetext{
    Random variables translate outcomes in the sample space of an experiment to
    real numbers so that we can reason about them with mathematics.}
\end{definition}

\begin{note}
  It is worth clarifying that ``random variable'' is a misnomer.
  A random variable is a function.
  It is not a variable, and it is not random!
  It is a representation of an underlying random system.
  The name \textit{random variable} (the translation of
  \textit{variabile casuale}) was introduced by the Italian mathematician
  F. P. Cantelli in 1916 \citep{ros02}.
\end{note}

\section{Discrete Probability Distributions}

\begin{definition}
  The \textbf{distribution} of a random variable $X$ on a sample space $S$
  is the set of pairs $(r,p(X=r))$ for all $r \in X(S)$, where $p(X=r)$ is the
  probability that $X$ takes the value $r$.
  The set of pairs in this distribution is determined by the probabilities
  $p(X=r)$ for $r \in X(S)$ \citep{ros02}.
\end{definition}

\subsection{Bernoulli Distribution}

\begin{definition}
  Discrete probability distribution of a random variable which takes the value
  $1$ with probability $p$ and the value $0$ with probability $q=1-p$.
\end{definition}

\paragraph{Random variable}
\begin{equation*}
  X = \text{The outcome of the \textbf{Bernoulli experiment} is ``success''.}
\end{equation*}

\paragraph{Probability mass function}
\begin{equation*}\label{bernoulli}
  f(k;p) = \mathrm{Pr}(k;p) = \mathrm{Pr}(X=k) = p^k(1 - p)^{1-k}
  \quad \text{for}\ k \in \{0,1\}
\end{equation*}

\paragraph{Parameter}
\begin{equation*}
  p = \text{Probability of the outcome of a \textbf{Bernoulli experiment}
    being a success.}
\end{equation*}

\subsection{Binomial Distribution}

\begin{definition}
  Discrete probability distribution of the number of successes in a sequence of
  $n$ independent Bernoulli experiments.
  For a single trial, that is, $n=1$, the binomial distribution is a Bernoulli
  distribution.
\end{definition}

\begin{note}
  The binomial distribution is frequently used to model the number of successes
  in a sample of size $n$ drawn with replacement from a population of size
  $N$.
  If the sampling is carried out without replacement, the draws are not
  independent and so the resulting distribution is a \textbf{hypergeometric}
  distribution, not a binomial one. However, for $N$ much larger than $n$, the
  binomial distribution remains a good approximation, and is widely used.
\end{note}

\paragraph{Random variable}
\begin{equation*}
  X = \text{The number of successes in a sequence of $n$ independent
    \textbf{Bernoulli experiments} is $k$.}
\end{equation*}

\paragraph{Probability mass function} of getting $k$ successes in $n$ trials.
\begin{equation*}
  f(k;n,p) = \mathrm{Pr}(k;n,p) = \mathrm{Pr}(X=k) = \binom{n}{k}p^k(1-p)^{n-k}
  \quad \text{for}\ k \in \{0,1,\dots,n\}
\end{equation*}

\paragraph{Parameters}
\begin{align*}
  p & = \text{Probability of the outcome of a \textbf{Bernoulli experiment}
    being a success.} \\
  n & = \text{Number of trials (experiemnts).}
\end{align*}

\begin{definition}
  The binomial coefficient, $\binom{n}{k} = \frac{n!}{k!(n-k)!}$, is the
  coefficient of the $x^k$ term in the polynomial expansion of the binomial
  power
  $(1+x)^n = \binom{n}{0}x^0 + \binom{n}{1}x^1 + \cdots + \binom{n}{n}x^n$,
  that is, the \textbf{binomial theorem}.
\end{definition}

\paragraph{Explanation}
$k$ successes occur with probability $p^k$ and $n-k$ failures occur with
probability $(1-p)^{n-k}$.
The $k$ successes can occur anywhere among the $n$ trials, and there are
$\binom{n}{k}$ different ways of distributing $k$ successes in a sequence of
$n$ trials.

\subsection{Poisson Distribution}

\begin{definition}
  Discrete probability distribution of a given number of events occurring in a
  fixed space interval, usually time, and these events occur with a known
  constant rate and independently of the time since the last event.
  The Poisson distribution is derived from the \textbf{binomial distribution}
  by splitting up the interval into $n$ subintervals, each of which is so small
  that at most one event could occur in it with non-zero probability
  \citep{wacmensch08}.
\end{definition}

\paragraph{Random variable}
\begin{equation*}
  X = \text{The number of events in an interval is $k$.}
\end{equation*}

\paragraph{Probability mass function} of getting $k$ events in an interval.
\begin{equation*}
  f(k;\lambda) = \mathrm{Pr}(k;\lambda) = \mathrm{Pr}(X=k) =
  e^{-\lambda}\frac{\lambda^k}{k!}
\end{equation*}

\paragraph{Parameter}
\begin{equation*}
  \lambda = \text{event rate, the average number of events in an interval.}
\end{equation*}

\subsection{Geometric Distribution}

\paragraph{Geometric random variable} has two alternate formulations:
\begin{align*}
  X_1 & = \text{The number of \textbf{Bernoulli experiments} needed to get the
        first success.} \\
  X_2 & = \text{The number of failed \textbf{Bernoulli experiments} before the
        first success.}
\end{align*}

\paragraph{Probability mass function} for each corresponding random variable.
\begin{align*}
  f_1(k;p) & = \mathrm{Pr}_1(k;p) = \mathrm{Pr}(X_1=k) = (1-p)^{k-1}p
           \qquad \text{for}\ k=1,2,\dots, 0 \leq p \leq 1 \\
  f_2(k;p) & = \mathrm{Pr}_2(k;p) = \mathrm{Pr}(X_2=k) = (1-p)^kp
           \qquad \text{for}\ k=0,1,2,\dots, 0 \leq p < 1
\end{align*}

\paragraph{Parameter}
\begin{equation*}
  p = \text{Probability of the outcome of a Bernoulli experiment being success.}
\end{equation*}

\paragraph{Example}

Probability that the first coin flip turns up heads:
\begin{equation*}
  p(X_1=1) = (1-p)^{1-1}p = (1-p)^0p = p(X_2=0)
\end{equation*}

\section{Continuous Probability Distributions}

\subsection{Exponential Distribution}

\begin{definition}
  The exponential distribution (also known as negative exponential distribution)
  is the probability distribution that describes the time between events in a
  \textbf{Poisson point process}.
\end{definition}
\begin{note}
  It is the continuous analogue of the \textbf{geometric distribution}, and it
  has the key property of being memoryless.
  In addition to being used for the analysis of Poisson point processes it is
  found in various other contexts.
\end{note}

\paragraph{Exponential random variable}
\begin{equation*}
  X = \text{The amount of time that passes before the next event.}
\end{equation*}

\paragraph{Probability density function}
\begin{equation*}
  f(x;\lambda) = \lambda e^{-\lambda x}\quad \text{for}\ x \geq 0
\end{equation*}

\paragraph{Parameter}
\begin{equation*}
  \lambda = \text{event rate, the average number of events in an interval.}
\end{equation*}

\paragraph{Derivation} from the \textbf{geometric distribution} \citep{hol18}.
\begin{enumerate}
\item Split up each interval into $n$ sub-intervals such that the probability
  of an event occurring in a certain sub-interval is $p = \frac{\lambda}{n}$
\item For a \textbf{geometric random variable}
  $Y = \text{The number of trials needed to get the first success}$,
  $p(X \leq b) \approx P(Y \leq b \cdot n)$
\begin{align*}
  P(Y \leq b \cdot n) & = \sum_{k=1}^{b \cdot n}P(Y=k) \\
                      & = \sum_{k=1}^{b \cdot n}(1-p)^{k-1}p =
                        \sum_{k=1}^{b \cdot n}\left(1-\frac{\lambda}{n}\right)^{k-1}
                        \cdot \frac{\lambda}{n} \\
                      & = \frac{\lambda}{n} \cdot \sum_{k=1}^{b \cdot n}
                        \left(1-\frac{\lambda}{n}\right)^{k-1} \\
                      & = \frac{\lambda}{n} \cdot \sum_{k=0}^{b \cdot n - 1}
                        \left(1-\frac{\lambda}{n}\right)^{k}
\end{align*}
\begin{align*}
  \sum_{k=0}^{m}a^k = \frac{1-a^{m+1}}{1-a}\quad
  \text{finite geometric sum} \\
  m = b \cdot n - 1 \text{and}\ a = 1 - \frac{\lambda}{n}
\end{align*}
\begin{align*}
  P(Y \leq b \cdot n) & =
                        \frac{\lambda}{n} \cdot
                        \frac{1-\left(1-\frac{\lambda}{n}\right)^{b \cdot n}}{1-\left(1-\frac{\lambda}{n}\right)} \\
  & = 1 - \left(1-\frac{\lambda}{n}\right)^{b \cdot n} \\
                      & = 1 - \left(\left(1-\frac{\lambda}{n}\right)^n\right)^b
\end{align*}
\begin{align*}
  p(X \leq b) & = \lim_{n \to \infty} P(Y \leq b \cdot n) \\
              & =
  \lim_{n \to \infty}\left(1 - \left(\left(1-\frac{\lambda}{n}\right)^n\right)^b\right) \\
              & = 1 -
  \left(\lim_{n \to \infty}\left(1-\frac{\lambda}{n}\right)^n\right)^b \\
              & = 1 - \left(e^{-\lambda}\right)^b \\
              & = 1 - e^{-\lambda\cdot b}
\end{align*}
\begin{align*}
  p(X \leq b) & = \int_{\infty}^b \mathrm{pdf}(x)\mathrm{d}x \\
  p(X \leq b) & = \int_0^b \mathrm{pdf}(x)\mathrm{d}x \\
  \frac{\mathrm{d}}{\mathrm{d}b} P(X \leq b) & = \frac{\mathrm{d}}{\mathrm{d}b}
                                               \int_o^b\mathrm{pdf}(x)\mathrm{d}x \\
                                             & = \mathrm{pdf}(b)
\end{align*}
\begin{align*}
\mathrm{pdf}(b) & = \frac{\mathrm{d}}{\mathrm{d}b} P(X \leq b) \\
                & = \frac{\mathrm{d}}{\mathrm{d}b}
                  \left(1-e^{-\lambda\cdot b}\right) \\
  & = \lambda e^{-\lambda b}
\end{align*}

\end{enumerate}
\cite{yue18} present an alternate derivation from a \textbf{Markov process}
representation of the exponential distribution.

\section{Stochastic (Random) Process}

\begin{definition}
  A \textbf{stochastic process} is a random phenomenon that arises through a
  process which is developing in time in a manner controlled by probabilistic
  laws \citep{par99}.
  From a point of view of the mathematical theory of probability a stochastic
  process is best defined as a collection of random variables defined on a
  common probability space $(\Omega,\mathcal{F}, P)$ and indexed by points in some space.
  \begin{equation*}
    \{X(t), t \in T\}
  \end{equation*}
  A more appropriate name in mathematics is \textbf{random field}.
  The set used to index the random variables is called the \textbf{index set}.
  Historically, the index set was some subset of the real line, such as the
  natural numbers, giving the index set the interpretation of time.
  Each random variable in the collection takes values from the same mathematical
  space known as the \textbf{state space}.
  An \textbf{increment} is the amount that a stochastic process changes between
  two index values, often interpreted as two points in time.
\end{definition}

\begin{note}
  Even though the index set can be any set in any space, generally more results
  and theorems are possible  for stochastic processes when the index set is
  ordered.
  Most commonly, random variables in a stochastic process are indexed by the
  positive numbers along the real number line, interpreted as time.
  Among these, the \textbf{Brownian motion process} and
  the \textbf{Poisson process} are considered the most important and central in
  the theory of stochastic processes.
  Other stochastic processes include \textbf{random walk} and
  \textbf{Markov Chain} and \textbf{martingales}.
  A martingale models a fair game.
  A simple random walk is a Markov chain and also a martingale.
\end{note}

\section{Point Process}

\begin{definition}
  Point processes are not defined like stochastic processes, but are used to
  describe data that are localized in space or time.
  Point processes on the real line form an important special case that is
  particularly amenable to study because the points are ordered in a natural
  way, and the whole point process can be described completely by the (random)
  intervals between the points or by the event times or by the event counts
  within each non-overlapping intervals.
  A \textbf{temporal} point process is a random process whose realization is a
  collection of points in time.
\end{definition}

\subsection{Renewal (Point) Process}

\begin{definition}
  Historically the first point processes that were studied had the real half
  line $\mathbb{R}_+ = [0,\infty)$ as their state space, which in this context
  is usually interpreted as time.
  These studies were motivated by the wish to model telecommunication systems,
  in which the points represented events in time, such as calls to a telephone
  exchange.
  Point processes on $\mathbb{R}_+$ are typically described by giving the
  sequence of their (random) inter-event times $(T1,T2,\dots)$, from which the
  actual sequence $(X1,X2,\dots)$ of event times can be obtained as
  \begin{equation*}
    X_k = \sum_{j=1}^{k} T_j\quad \text{for}\ k \geq 1.
  \end{equation*}
  If inter-event times are independent and identically distributed, the point
  process is called a \textbf{renewal process}.
\end{definition}

\begin{note}
  The \textbf{intensity} $\lambda(t|H_t)$ of a point process on the real
  half-line with respect to a filtration $H_t$ is defined as
  \begin{equation*}
    \lambda(t|H_t) = \lim_{\Delta t \to 0} \frac{1}{\Delta t} \mathrm{Pr}
    (\text{One event occurs in time interval} [t, t+\Delta t] | H_t)
  \end{equation*}
  $H_t$ can denote the history of event-point times preceding time $t$ but can
  also correspond to other \emph{filtrations} (for example in the case of a Cox
  process).
  The \emph{compensator} of a point process, also known as the dual-predictable
  projection, is the integrated conditional intensity function defined by
  \begin{equation*}
    \Lambda(s,u) = \int_s^u \lambda(t|H_t)\mathrm{d}t
  \end{equation*}
\end{note}

% \footnote is a fragile command and section (or subsection) title are moving
% arguments (meaning they get written to an auxiliary file to be used in the
% table of contents). Fragile commands break in moving arguments and the
% standard solution is to protect them with \protect. However, this will produce
% a footnote in the table of contents. In order to avoid this, use the optional
% argument of \section for the TOC or even better, do not use footnotes in
% headings.
% https://tex.stackexchange.com/questions/153329/footnote-in-sub-section-title
\subsection[Poisson Process]{Poisson (Point)\footnotemark\ Process}

\footnotetext{
  The word point is often omitted, but there are other Poisson processes of
  objects, which, instead of points, consist of more complicated mathematical
  objects such as lines and polygons, and such processes can be based on the
  Poisson point process.
}

\begin{definition}
  The number of points in a region of finite size within a Poisson process is
  a random variable with a \textbf{Poisson distribution}.
  The Poisson point process is often defined on the real line, where it can be
  considered as a \textbf{stochastic process}.
  The Poisson process is a point process with convenient mathematical
  properties.
  The Poisson point process has the property that each point is stochastically
  independent to all the other points in the process.
  The Poisson process has been used to build other point processes where the
  points are not independent of each other.
\end{definition}

\begin{definition}
  \textbf{Poisson Counting Process} on the positive half-line that represents
  the total number of occurrences or events that have happened up to and
  including time $t$.
  \begin{equation*}
    \{ N(t), t \geq 0 \}
  \end{equation*}
  The probability of random variable $N(t)$ being equal to $n$:
  \begin{equation*}
    P\{N(t)=n\} = e^{\lambda t} \frac{(\lambda t)^n}{n!}
  \end{equation*}
\end{definition}

\begin{definition}
  \textbf{Poisson Point Process} on the positive half-line that represents
  the number of occurrences in the interval $(a,b]$.
  \begin{equation*}
    P\{N(a_i,b_i] = n_i,\ i=1,\dots,k\} =
    \prod_{i=1}^k\frac{[\lambda(b_i-a_i)]^{n_i}}{n!}e^{-\lambda(b_i-a_i)}
  \end{equation*}
\end{definition}

\begin{definition}
  \textbf{Poisson Point Process}\footnotemark\ on the positive half-line that
  represents the inter-event times.
  \footnotetext{This is the interpretation that we will use when describing PCIM.}
  \begin{equation*}
    P\{X_i,\ i=1,\dots,k\} =
    \prod_{i=1}^k\lambda e^{-\lambda x_i}
  \end{equation*}
\end{definition}

\subsection{Marked Point Process}

\begin{definition}
  Let $\mathcal{X}$ be the space of random variables of an ordinary point
  process and $\mathcal{L}$ be the space of random variables of labels.
  A marked point process can be regarded as either a point process in the
  product space $\mathcal{X} \times \mathcal{L}$, subject to the finiteness
  constraint of the ground process (that is, the point process of event times),
  or as an ordinary point process in $\mathcal{X}$ with an associated sequence
  random variables taking their values in $\mathcal{L}$.
  A realization of a marked point process is a sequence $\{(x_i,l_i)\}_{i=1}^n$.
\end{definition}

\subsection{PCIM}

  The \textbf{Piecewise-Constant Conditional Intensity Model} (PCIM) is a class
  of \textbf{marked point processes} that can model the types and timing of
  events.
  This model captures the dependencies of each type of event on events in the
  past through a set of piecewise-constant conditional intensity functions.
  Decision trees represent these dependencies. The decision trees are the
  picewise-constant conditional intensity functions \citep{gunmeexu11}

  A conjugate prior for this model allows for closed-form computation of the
  marginal likelihood and parameter posteriors.
  Model selection then becomes a problem of choosing a decision tree.
  Decision tree induction can be done efficiently because of the closed form for
  the marginal likelihood.

  \textbf{Poisson Networks} \citep{rajgraher05} (also piece-wise constant
  conditional intensity models) are closely related to PCIMs.

\begin{definition}
  \textbf{Event stream}
\begin{equation*}
  y = \{(t_i,l_i)\}_{i=1}^n
\end{equation*}
with $0 < t_1 < \cdots < t_n$, where $t_i \in [0, \infty)$ is the time of the
$i$th event and $l_i$ is its label, drawn from a finite label set $\mathcal{L}$.
\end{definition}

\begin{definition}
  \textbf{History at time $t$} of event sequence $y$ is the sub-sequence
\begin{equation*}
  h(t,y) = \{(t_i,l_i)|(t_i,l_i) \in y,t_i \leq t\}
\end{equation*}
\begin{itemize}
  \item $t_0 = 0$
  \item $h_i = h(t_{i-1},y)$ is the history at time $t_i$
  \item $t(y) = \max(\{t:(t,l) \in y\})$ is the end time of event
    sequence $y$ such that $t(h_i) = t_{i-1}$
\end{itemize}
\end{definition}

\begin{definition}
  \textbf{Conditional Intensity Model} \citep{did08,dalver09}:
\begin{equation*}
  p(x|\theta) = \prod_{l \in \mathcal{L}} \prod_{i=1}^{n}
  \lambda_l(t_i|h_i,\theta)^{\mathbf{1}_l(l_i)}
  e^{-\Lambda_l(t_i|h_i;\theta)}
\end{equation*}
where
$\Lambda_l(t|x;\theta) = \int_{-\infty}^t\lambda_l(\tau|x;\theta)\mathrm{d}t$
for each event sequence $x$ and the indicator function $\mathbf{1}_l(l')$ is one
if $l'=l$ and zero otherwise.
The rate parameter changes as a function of time and history and is computed by
the conditional intensity function $\lambda_l(t_i|h_i,\theta)$.
``The conditional intensities are assumed to satisfy $\lambda_l(t|x;\theta) = 0$
for $t \leq t(x)$ to ensure that $t_i > t_{i-1}=t(h_i)$.''\footnote{This
  sentence means that the conditional intensity functions will only evaluate
  to valid values for times into the future. ``$\lambda_l(t|y)$ is piecewise
  constant in $t$ for all $t > t(y)$'' \citep{pargunmee12}.}
\end{definition}

\begin{note}
  These models offer a powerful approach for decomposing the dependencies of
  different event types on the past.
  In particular, this per-label conditional factorization allows one to model
  detailed label-specific dependence on past events \citep{gunmeexu11}.
\end{note}

\begin{note}
  The \emph{Conditional Intensity Model} is analogous\footnotemark\ to the
  likelihood of a sequence of samples (\textbf{inter-event times}) from the
  exponential distribution where the rate parameter changes over time.
  Let $X$ be exponential distributed with rate parameter $\lambda$ and
  $x_1,\dots,x_n$ are $n$ independent samples from $X$.
  The likelihood function is given by:
  \begin{equation*}
    L(\lambda;x) = p(x;\lambda) = \prod_{i=1}^n\lambda e^{-\lambda x_i}
  \end{equation*}
  When there is only one label, the conditional intensity model becomes
  \begin{equation*}
    p(x|\theta) = \prod_{i=1}^n\lambda(t_i|h_i,\theta)
    e^{-\Lambda(t_i|h_i;\theta)}
  \end{equation*}
  and when the rate parameter is fixed $\lambda(t_i|h_i,\theta) = \lambda$ it
  becomes $L(\lambda;x)$.
\end{note}

\footnotetext{
  Despite the similarity to the likelihood of a non-homogeneous Poisson process,
  this likelihood does not in general define a Poisson process as the
  conditioning on history can cause the independent increments property of
  Poisson processes to not hold.
}

\begin{definition}
  \textbf{Piecewise-Constant Conditional Intensity Models (PCIM)} are
  a factorization of Conditional Intensity Models where the conditional
  intensity functions are assumed to be piecewise-constant. This assumption
  allows efficient learning and inference.
  \begin{equation*}
    p(x|S,\Theta) = \prod_{l\in\mathcal{L}}\prod_{s\in\Sigma_l}\lambda_{ls}^{c_{ls}(x)}
    e^{-\lambda_{ls}d_{ls}(x)}
  \end{equation*}
  \begin{itemize}
    \item $S=\{S_l\}_{l\in\mathcal{L}}$, where $S_l=\left(\Sigma_l,\sigma_l(t,x)\right)$
      : Set of local structures, one per event type (label $l$)
    \item $\Sigma_l$ is a set of discrete \textbf{states}
    \item $s=\sigma_l(t,x)$ is the state given by the
      \textbf{piecewise-constant state function} (a \textbf{decision tree})
    \item $\Theta=\{\theta_l\}_{l\in\mathcal{L}}$,
      where $\theta_l=\{\lambda_{ls}\}_{s\in\Sigma_l}$: Set of local parameters
    \item $\lambda_l(t|x)=\lambda_{ls}$:
      The \textbf{piecewise-constant conditional intensity functions}
    \item $c_{ls}(x)$: The number of times label $l$ occurs in state $s$
    \item $d_{ls}(x)$: The total time during which $\sigma_l(t|x)$ maps to state $s$
  \end{itemize}
\end{definition}

\begin{note}
  ``The state $s$ summarizes all the information about $t$ and $y$ necessary for
  computing $\lambda_l(t|y)$ in that given $s$, $\lambda_l(t|y)$ can be
  computed without further information about $t$ and $y$.
  However, unlike in Markov models such as CTBNs, the state does not contain
  all the information about $t$ and $y$ for predicting future states.''
  \citep{pargunmee12}
\end{note}

\begin{note}
  \cite{gunmeexu11} show that given the structure S, a product of Gamma
  distributions is a conjugate prior for $\Theta$, and that under this prior,
  the marginal likelihood of the data can be given in closed form.
  Thus, parameter estimation can be done in closed form given a structure, and
  imposing a structural prior allows a closed form Bayesian score to be computed
  for a structure.
\end{note}

\begin{note}
  \cite{gunmeexu11} demonstrate their method on two problems. The forecast the
  user's web search queries and failure events based on system logs on a
  machine.
\end{note}

\subsection{C-PCIM}

\begin{definition}
  \textbf{Conjoint Piecewise-Constant Conditional Intensity Models} generalize
  PCIMs by allowing the sharing of parameters across event types. This parameter
  sharing allows to better model event streams of fine-grained events, which
  are likely to be rare events.
\end{definition}

\begin{note}
  In contrast, PCIMs learn dependencies of each event type separately, using
  independent sub-models for each event type. If some event types are rare,
  there would not be sufficient data to learn an independent sub-model for
  such types.
\end{note}

\begin{note}
  During structure learning, the C-PCIM learns what event types in what
  historical contexts can be modeled by shared parameters, thereby allowing more
  efficient use of data during parameter estimation. In cases where events are
  structured (encoded via the basis test functions?), with the different event
  types having known attributes, we show how structure learning can take
  advantage of these attributes to distinguish between different event types
  when their dependencies differ, while sharing parameters when they do not.
\end{note}

\begin{definition}
  \textbf{Conjoint Piecewise-Constant Conditional Intensity Models (C-PCIM)} are
  PCIMs where the parameters are shared across labels by a single state function
  $\sigma(l,t,y)$ that now also depends on the label $l$.
  \begin{equation*}
    p(x|S,\Theta) = \prod_{s\in\Sigma}\lambda_{s}^{c_{s}(x)}
    e^{-\lambda_{s}d_{s}(x)}
  \end{equation*}
  \begin{itemize}
    \item $S=\left(\Sigma,\sigma(\cdot,\cdot,\cdot)\right)$: Structure
    \item $\Sigma$: Set of discrete \textbf{states}
    \item $s=\sigma(l,t,x)$ is the state given by the
      \textbf{piecewise-constant state function} (a \textbf{decision tree})
    \item $\Theta=\{\lambda_s\}_{s\in\Sigma}$: Set of parameters shared across
      labels $l \in \mathcal{L}$
    \item $\lambda_l(t|x)=\lambda_{s}$:
      The \textbf{piecewise-constant conditional intensity functions}
    \item $c_{s}(x)= \sum_i\mathbf{1}_s(\sigma(l_i,t_i,h_i))$:
      The number of times label $l$ occurs in $x$ when the state function maps
      to state $s$ for label $l$
    \item $d_{s}(x) = \sum_{i}\mathbf{1}_s(\sigma(l_{i-1},t_{i-1},h_{i-1}))(t_i-t_{i-1})$
      : The total time spent in state $s$
  \end{itemize}
\end{definition}

\paragraph{Parameter Learning}
Using a product of Gamma priors on $\lambda_s$ as conjugate for $\Theta$,
the point estimate $\hat{\Theta} = \mathbf{E}[\Theta|S,x]$, given a structure
and the data, is given by
\begin{equation*}
  \hat{\lambda}_s = \frac{\alpha + c_s(x)}{\beta + d_s(x)}
\end{equation*}

\paragraph{Structure Learning}
Using a factored prior on the structure, $p(S) \propto \kappa^{|\Sigma|}$,
learn a decision tree \citep{chihecmee97} by selecting the next node on the tree
to split based on the gain:
\begin{equation*}
  \mathrm{Gain}(S \to S') =
  \frac{\prod_{s'\in\Sigma_f}\kappa\gamma_{s \odot s'}(x)}{\kappa\gamma_s(x)}
\end{equation*}
where
\begin{equation*}
  \gamma_s(x) =
  \frac{\beta^{\alpha}}{\Gamma(\alpha)}\frac{\Gamma(\alpha + c_s(x))}{(\beta + d_s(x))^{\alpha + c_s(x)}}
\end{equation*}
where the gamma function, $\Gamma(\cdot)$, is an extension of the factorial
function to real and complex numbers
(it is not defined on the negative integers):
\begin{align*}
  \Gamma(n) & = (n-1)!,\ n\in\mathbb{Z}^+ \\
  \Gamma(z) & = \int_0^{\infty}x^{z-1}e^{-x}\mathrm{d}x
\end{align*}

\begin{algorithm}[H]
%\TitleOfAlgo{Structure Learning}
\DontPrintSemicolon
\KwIn{$\mathcal{B}=\{f_i(l,t,y)\}_{i=k},X = \{(T_i,l_i)\}_{i=1}^n$}
\SetKwInOut{Parameters}{Parameters}
\Parameters{$\alpha,\beta,\kappa$}
\SetKwInOut{Initialization}{Initialization}
\KwOut{$S=(\Sigma,\sigma(\cdot,\cdot,\cdot))$}
\SetKwFunction{Gain}{Gain}
\SetKwFunction{AddTreeNode}{AddTreeNode}
\SetKwFunction{DecisionTreeLearning}{DecisionTreeLearning}
\Initialization{$\Sigma=\{s_0\},\sigma(l,t,y)=s_0$}
\SetKw{Return}{return}
\Begin{
  $S \longleftarrow \DecisionTreeLearning(\mathcal{B},X,\Gain,\alpha,\beta,\kappa))$
}
\SetKwProg{Def}{def}{:}{}
\Def{$DecisionTreeLearning(\mathcal{B},X,\Gain,\alpha,\beta,\kappa)$}{
  \Repeat{$\Gain(S\to S')\leq1$}
  {$S' \longleftarrow \AddTreeNode(S)$;}
  \Return{$S$}
}
\caption{Structure Learning}
\label{algo:structure}
\end{algorithm}

\begin{algorithm}[H]
%\TitleOfAlgo{Parameter Learning}
\DontPrintSemicolon
\KwIn{$S=(\Sigma,\sigma(\cdot,\cdot,\cdot))$}
\SetKwInOut{Parameters}{Parameters}
\Parameters{$\alpha,\beta$}
\KwOut{$\Theta=\{\lambda_s\}_{s\in\Sigma}$}
\SetKw{Return}{return}
\SetKwFor{FOR}{for}{do}{end}
\Begin{
  \FOR{$s\in\Sigma$}
  {$\lambda_s=\frac{\alpha+c_s(x)}{\beta+d_s(x)}$;}
}
\caption{Parameter Learning}
\label{algo:parameters}
\end{algorithm}

\bibliography{bibliography}
\bibliographystyle{plainnat}
\end{document}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% End:
